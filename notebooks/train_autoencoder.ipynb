{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44679ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.utils import shuffle\n",
    "from keras.preprocessing import sequence, image\n",
    "from keras.preprocessing.image import array_to_img, save_img, img_to_array\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from keras import metrics\n",
    "from keras.layers import (\n",
    "    Flatten,\n",
    "    Dense,\n",
    "    Input,\n",
    "    Activation,\n",
    "    BatchNormalization,\n",
    "    Conv2D,\n",
    "    MaxPool2D,\n",
    "    Dropout,\n",
    "    UpSampling2D,\n",
    "    Lambda,\n",
    ")\n",
    "\n",
    "import pickle\n",
    "from keras.layers import ReLU, Reshape, Conv2DTranspose, Concatenate, Multiply\n",
    "from keras.models import Model\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.losses import binary_crossentropy, categorical_crossentropy\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from collections import Counter\n",
    "from keras import metrics\n",
    "from notebook_utils import initialize_environment\n",
    "initialize_environment()\n",
    "\n",
    "from utils.data_loading.load_data import get_tile_data\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from ast import literal_eval\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0dc52985",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = \"../data/context_data/\"\n",
    "json_directory = \"../data/json_files_trimmed_features/\"\n",
    "\n",
    "game_data_path=\"../data/game_data.csv\"\n",
    "train_data_path=\"../data/train_data.csv\"\n",
    "test_data_path=\"../data/test_data.csv\"\n",
    "\n",
    "EPOCHS=10\n",
    "BATCH_SIZE=25\n",
    "LATENT_DIM=256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02b293d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func_image(y_true, y_pred):\n",
    "    # tile sprite loss\n",
    "    r_loss=K.mean(K.square(y_true - y_pred), axis=[1,2,3])\n",
    "    loss  =  r_loss\n",
    "    return loss\n",
    "    \n",
    "def loss_func_text(y_true,y_pred):\n",
    "    # multilabel text weighted bce\n",
    "    y_true = K.cast(y_true, 'float32')\n",
    "    y_pred = K.cast(y_pred, 'float32')\n",
    "    bce_array=-(y_true*K.log(y_pred)+(1-y_true)*K.log(1-y_pred))\n",
    "    weighted_array=bce_array*tensor_from_list\n",
    "    bce_sum=K.sum(weighted_array,axis=1)\n",
    "    loss=bce_sum/13.0\n",
    "    return loss\n",
    "\n",
    "def check_nonzero(y_true,y_pred):\n",
    "    \"\"\"\n",
    "    Custom metric\n",
    "    Returns sum of all embeddings\n",
    "    \"\"\"\n",
    "    return(K.sum(K.cast(y_pred > 0.4, 'int32')))\n",
    "\n",
    "def get_pickle_file(path):\n",
    "    with open(path,\"rb\") as handle:\n",
    "        return pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11ec3c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Dictionary Loaded\n",
      "The feature dictionary has size 13\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Features'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array(['block', 'breakable', 'climbable', 'collectable', 'element',\n",
       "       'empty', 'hazard', 'moving', 'openable', 'passable', 'pipe',\n",
       "       'solid', 'wall'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\Training Testing Batches loaded\n",
      "Train Image batch shape (18494, 48, 48, 3)\n",
      "Train Text batch shape (18494, 13)\n",
      "Train Output Image batch shape (18494, 16, 16, 3)\n",
      "Train Output Text batch shape (18494, 13)\n",
      "Test Image batch shape (2055, 48, 48, 3)\n",
      "Test Text batch shape (2055, 13)\n"
     ]
    }
   ],
   "source": [
    "# data loading\n",
    "data=pd.read_csv(game_data_path)\n",
    "train_data=pd.read_csv(train_data_path)\n",
    "test_data=pd.read_csv(test_data_path)\n",
    "\n",
    "data['features'] = data.features.apply(lambda x: literal_eval(str(x)))\n",
    "train_data['features'] = train_data.features.apply(lambda x: literal_eval(str(x)))\n",
    "test_data['features'] = test_data.features.apply(lambda x: literal_eval(str(x)))\n",
    "\n",
    "# loading multi-label binarizer\n",
    "mlb=get_pickle_file(\"../model/model_tokenizer.pickle\")\n",
    "print(\"Feature Dictionary Loaded\")\n",
    "total_features = len(mlb.classes_)\n",
    "print(\"The feature dictionary has size\", total_features)\n",
    "display(\"Features\", mlb.classes_)\n",
    "\n",
    "# loading the batches\n",
    "# training \n",
    "train_image_batch=get_pickle_file(\"../data/train_image_batch.pickle\")\n",
    "train_text_batch=get_pickle_file(\"../data/train_text_batch.pickle\")\n",
    "output_image_batch=get_pickle_file(\"../data/output_image_batch.pickle\")\n",
    "output_text_batch=get_pickle_file(\"../data/output_text_batch.pickle\")\n",
    "\n",
    "#testing\n",
    "test_image_batch=get_pickle_file(\"../data/test_image_batch.pickle\")\n",
    "test_text_batch=get_pickle_file(\"../data/test_text_batch.pickle\")\n",
    "\n",
    "print(\"\\Training Testing Batches loaded\")\n",
    "\n",
    "print(\"Train Image batch shape\", train_image_batch.shape)\n",
    "print(\"Train Text batch shape\", train_text_batch.shape)\n",
    "print(\"Train Output Image batch shape\", output_image_batch.shape)\n",
    "print(\"Train Output Text batch shape\", output_text_batch.shape)\n",
    "\n",
    "print(\"Test Image batch shape\", test_image_batch.shape)\n",
    "print(\"Test Text batch shape\", test_text_batch.shape)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "476bdaa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Printing the TF-IDF for the labels\n",
      "\n",
      " {'block': 4.895666560876659, 'breakable': 2.527712174910271, 'climbable': 2.7397689321637086, 'collectable': 4.067161199838823, 'element': 5.357195563131423, 'empty': 2.0458521063722053, 'hazard': 4.6853711520402985, 'moving': 5.965443299904882, 'openable': 6.220085518278463, 'passable': 1.5640321219758018, 'pipe': 6.190526716036919, 'solid': 1.8879058559991626, 'wall': 5.050704158722146}\n",
      "\n",
      "Printing the weight normalised\n",
      "\n",
      "\n",
      "{'block': 0.09202826128752296, 'breakable': 0.04751568629106969, 'climbable': 0.051501908477902945, 'collectable': 0.07645409852631353, 'element': 0.10070403834119256, 'empty': 0.038457727841484414, 'hazard': 0.08807514875600009, 'moving': 0.11213781981942068, 'openable': 0.11692455934016166, 'passable': 0.029400522889674464, 'pipe': 0.11636891586603913, 'solid': 0.03548866967178095, 'wall': 0.09494264289143667}\n",
      "Metal device set to: Apple M1\n",
      "Weight Vector\n",
      "[4895.666560876659, 2527.7121749102707, 2739.7689321637085, 4067.1611998388234, 5357.195563131423, 2045.8521063722053, 4685.3711520402985, 5965.443299904882, 6220.085518278463, 1564.0321219758018, 6190.5267160369185, 1887.9058559991627, 5050.7041587221465]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-09 07:31:19.752736: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2021-09-09 07:31:19.752840: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "#initialise the TF-IDF vectorizer to counter imbalanced dataset\n",
    "vectorizer = TfidfVectorizer(stop_words=None)\n",
    "train_data_copy = train_data\n",
    "train_data_copy[\"features\"] = train_data_copy.features.apply(lambda x: str(x))\n",
    "vectors = vectorizer.fit_transform(train_data_copy[\"features\"])\n",
    "idf = vectorizer.idf_\n",
    "new_dict = {}\n",
    "for c in mlb.classes_:\n",
    "    if c in vectorizer.vocabulary_.keys():\n",
    "        new_dict[c] = idf[vectorizer.vocabulary_[c]]\n",
    "    else:\n",
    "        new_dict[c] = np.max(idf)\n",
    "print(\"\\n Printing the TF-IDF for the labels\\n\\n\", new_dict)\n",
    "weight_freq = {k: v / sum(new_dict.values()) for k, v in new_dict.items()}\n",
    "print(\"\\nPrinting the weight normalised\\n\\n\")\n",
    "print(weight_freq)\n",
    "weight_vector = [v * 1000 for v in new_dict.values()]\n",
    "tensor_from_list = tf.convert_to_tensor(weight_vector)\n",
    "tensor_from_list = K.cast(tensor_from_list, \"float32\")\n",
    "print(\"Weight Vector\")\n",
    "print(weight_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b240e21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model definition\n",
    "latent_dim = 128\n",
    "batch_size = 1\n",
    "# image encoder\n",
    "image_encoder_input = Input(shape=(48, 48, 3), name=\"image_input\")\n",
    "image_encoder_conv_layer1 = Conv2D(\n",
    "    32, strides=3, kernel_size=(3, 3), name=\"iencode_conv1\"\n",
    ")(image_encoder_input)\n",
    "image_encoder_norm_layer1 = BatchNormalization()(image_encoder_conv_layer1)\n",
    "image_encoder_actv_layer1 = ReLU()(image_encoder_norm_layer1)\n",
    "image_encoder_conv_layer2 = Conv2D(32, (3, 3), padding=\"same\", name=\"iencode_conv2\")(\n",
    "    image_encoder_actv_layer1\n",
    ")\n",
    "image_encoder_norm_layer2 = BatchNormalization()(image_encoder_conv_layer2)\n",
    "image_encoder_actv_layer2 = ReLU()(image_encoder_norm_layer2)\n",
    "image_encoder_conv_layer3 = Conv2D(16, (3, 3), padding=\"same\", name=\"iencode_conv3\")(\n",
    "    image_encoder_actv_layer2\n",
    ")\n",
    "image_encoder_norm_layer3 = BatchNormalization()(image_encoder_conv_layer3)\n",
    "image_encoder_actv_layer3 = ReLU()(image_encoder_norm_layer3)\n",
    "image_shape_before_flatten = K.int_shape(image_encoder_actv_layer3)[1:]\n",
    "image_flatten = Flatten(name=\"image_flatten_layer\")(image_encoder_actv_layer3)\n",
    "\n",
    "# text encoder\n",
    "text_encoder_input = Input(shape=(13,))\n",
    "text_encoder_dense_layer1 = Dense(32, activation=\"tanh\", name=\"tencode_dense1\")(\n",
    "    text_encoder_input\n",
    ")\n",
    "text_encoder_dense_layer2 = Dense(16, activation=\"tanh\", name=\"tencode_dense2\")(\n",
    "    text_encoder_dense_layer1\n",
    ")\n",
    "text_shape_before_concat = K.int_shape(text_encoder_dense_layer2)[1:]\n",
    "\n",
    "# image-text concatenation\n",
    "image_text_concat = Concatenate(name=\"image_text_concatenation\")(\n",
    "    [image_flatten, text_encoder_dense_layer2]\n",
    ")\n",
    "image_text_concat = Dense(256, activation=\"tanh\", name=\"embedding_dense_1\")(\n",
    "    image_text_concat\n",
    ")\n",
    "\n",
    "\n",
    "# define encoder model\n",
    "encoding_model = Model(\n",
    "    inputs=[image_encoder_input, text_encoder_input], outputs=image_text_concat\n",
    ")\n",
    "\n",
    "# decoder for image\n",
    "# decoder_input=Input(shape=(512,))\n",
    "image_y = Dense(units=np.prod(image_shape_before_flatten), name=\"image_dense\")(\n",
    "    image_text_concat\n",
    ")\n",
    "image_y = Reshape(target_shape=image_shape_before_flatten, name=\"image_reshape\")(\n",
    "    image_y\n",
    ")\n",
    "image_decoder_convt_layer1 = Conv2DTranspose(\n",
    "    16, (3, 3), padding=\"same\", name=\"idecode_conv1\"\n",
    ")(image_y)\n",
    "image_decoder_norm_layer1 = BatchNormalization(name=\"idecode_norm1\")(\n",
    "    image_decoder_convt_layer1\n",
    ")\n",
    "image_decoder_actv_layer1 = ReLU(name=\"idecode_relu1\")(image_decoder_norm_layer1)\n",
    "image_decoder_convt_layer2 = Conv2DTranspose(\n",
    "    32, (3, 3), padding=\"same\", name=\"idecode_conv2\"\n",
    ")(image_decoder_actv_layer1)\n",
    "image_decoder_norm_layer2 = BatchNormalization(name=\"idecode_norm2\")(\n",
    "    image_decoder_convt_layer2\n",
    ")\n",
    "image_decoder_actv_layer2 = ReLU(name=\"idecode_relu2\")(image_decoder_norm_layer2)\n",
    "image_decoder_output = Conv2DTranspose(\n",
    "    3, (3, 3), padding=\"same\", name=\"image_output_layer\"\n",
    ")(image_decoder_actv_layer2)\n",
    "\n",
    "\n",
    "# decoder for text\n",
    "text_decoder_dense_layer1 = Dense(16, activation=\"tanh\", name=\"tdecode_dense1\")(\n",
    "    image_text_concat\n",
    ")\n",
    "text_reshape = Reshape(target_shape=text_shape_before_concat, name=\"text_reshape\")(\n",
    "    text_decoder_dense_layer1\n",
    ")\n",
    "text_decoder_dense_layer2 = Dense(32, activation=\"tanh\", name=\"tdecode_dense2\")(\n",
    "    text_reshape\n",
    ")\n",
    "text_decoder_output = Dense(13, activation=\"sigmoid\", name=\"text_output_layer\")(\n",
    "    text_decoder_dense_layer2\n",
    ")\n",
    "# decoding_model=Model(inputs=[decoder_input],outputs=[image_decoder_output,text_decoder_output])\n",
    "\n",
    "ae_sep_output = Model(\n",
    "    [image_encoder_input, text_encoder_input],\n",
    "    [image_decoder_output, text_decoder_output],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b877ac7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-09 07:31:28.619179: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2021-09-09 07:31:28.619410: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-09 07:31:37.838757: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "592/592 [==============================] - ETA: 0s - loss: 1689.8965 - image_output_layer_loss: 8858.6509 - text_output_layer_loss: 893.3685 - image_output_layer_loss_func_image: 8858.6512 - text_output_layer_check_nonzero: 59.9411"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-09 07:31:48.113921: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "592/592 [==============================] - 21s 19ms/step - loss: 1689.0060 - image_output_layer_loss: 8854.0172 - text_output_layer_loss: 892.8939 - image_output_layer_loss_func_image: 8854.0175 - text_output_layer_check_nonzero: 59.9233 - val_loss: 569.9338 - val_image_output_layer_loss: 2852.4338 - val_text_output_layer_loss: 316.3228 - val_image_output_layer_loss_func_image: 2852.4338 - val_text_output_layer_check_nonzero: 44.2432\n",
      "Epoch 2/10\n",
      "592/592 [==============================] - 11s 18ms/step - loss: 451.3461 - image_output_layer_loss: 2359.4762 - text_output_layer_loss: 239.3315 - image_output_layer_loss_func_image: 2359.4762 - text_output_layer_check_nonzero: 44.3932 - val_loss: 275.3939 - val_image_output_layer_loss: 1574.3854 - val_text_output_layer_loss: 131.0616 - val_image_output_layer_loss_func_image: 1574.3855 - val_text_output_layer_check_nonzero: 46.2297\n",
      "Epoch 3/10\n",
      "592/592 [==============================] - 11s 18ms/step - loss: 263.1439 - image_output_layer_loss: 1623.4912 - text_output_layer_loss: 111.9943 - image_output_layer_loss_func_image: 1623.4912 - text_output_layer_check_nonzero: 46.5777 - val_loss: 194.0154 - val_image_output_layer_loss: 1251.1981 - val_text_output_layer_loss: 76.5508 - val_image_output_layer_loss_func_image: 1251.1981 - val_text_output_layer_check_nonzero: 46.4122\n",
      "Epoch 4/10\n",
      "592/592 [==============================] - 11s 19ms/step - loss: 189.3197 - image_output_layer_loss: 1316.5263 - text_output_layer_loss: 64.0745 - image_output_layer_loss_func_image: 1316.5263 - text_output_layer_check_nonzero: 46.5432 - val_loss: 138.7811 - val_image_output_layer_loss: 970.1994 - val_text_output_layer_loss: 46.4012 - val_image_output_layer_loss_func_image: 970.1994 - val_text_output_layer_check_nonzero: 46.1622\n",
      "Epoch 5/10\n",
      "592/592 [==============================] - 11s 18ms/step - loss: 137.2051 - image_output_layer_loss: 1021.6048 - text_output_layer_loss: 38.9385 - image_output_layer_loss_func_image: 1021.6048 - text_output_layer_check_nonzero: 46.5079 - val_loss: 117.8040 - val_image_output_layer_loss: 920.4296 - val_text_output_layer_loss: 28.6233 - val_image_output_layer_loss_func_image: 920.4296 - val_text_output_layer_check_nonzero: 46.5068\n",
      "Epoch 6/10\n",
      "592/592 [==============================] - 11s 18ms/step - loss: 115.1013 - image_output_layer_loss: 925.1456 - text_output_layer_loss: 25.0964 - image_output_layer_loss_func_image: 925.1456 - text_output_layer_check_nonzero: 46.7198 - val_loss: 88.7267 - val_image_output_layer_loss: 705.7574 - val_text_output_layer_loss: 20.1678 - val_image_output_layer_loss_func_image: 705.7574 - val_text_output_layer_check_nonzero: 46.5000\n",
      "Epoch 7/10\n",
      "592/592 [==============================] - 11s 18ms/step - loss: 92.5284 - image_output_layer_loss: 774.2832 - text_output_layer_loss: 16.7779 - image_output_layer_loss_func_image: 774.2832 - text_output_layer_check_nonzero: 46.6418 - val_loss: 93.6500 - val_image_output_layer_loss: 789.6387 - val_text_output_layer_loss: 16.3179 - val_image_output_layer_loss_func_image: 789.6387 - val_text_output_layer_check_nonzero: 46.5203\n",
      "Epoch 8/10\n",
      "592/592 [==============================] - 11s 18ms/step - loss: 81.3672 - image_output_layer_loss: 698.8076 - text_output_layer_loss: 12.7628 - image_output_layer_loss_func_image: 698.8076 - text_output_layer_check_nonzero: 46.7475 - val_loss: 66.9995 - val_image_output_layer_loss: 562.9968 - val_text_output_layer_loss: 11.8887 - val_image_output_layer_loss_func_image: 562.9968 - val_text_output_layer_check_nonzero: 46.5338\n",
      "Epoch 9/10\n",
      "592/592 [==============================] - 11s 18ms/step - loss: 67.3362 - image_output_layer_loss: 581.3691 - text_output_layer_loss: 10.2215 - image_output_layer_loss_func_image: 581.3691 - text_output_layer_check_nonzero: 46.8407 - val_loss: 57.3726 - val_image_output_layer_loss: 482.6133 - val_text_output_layer_loss: 10.1236 - val_image_output_layer_loss_func_image: 482.6134 - val_text_output_layer_check_nonzero: 46.4459\n",
      "Epoch 10/10\n",
      "592/592 [==============================] - 11s 18ms/step - loss: 58.7479 - image_output_layer_loss: 511.6548 - text_output_layer_loss: 8.4249 - image_output_layer_loss_func_image: 511.6548 - text_output_layer_check_nonzero: 46.6550 - val_loss: 52.2388 - val_image_output_layer_loss: 456.4069 - val_text_output_layer_loss: 7.3312 - val_image_output_layer_loss_func_image: 456.4069 - val_text_output_layer_check_nonzero: 46.4865\n"
     ]
    }
   ],
   "source": [
    "losses ={'image_output_layer':loss_func_image,\n",
    "          'text_output_layer':loss_func_text,\n",
    "}\n",
    "#tweak loss weights\n",
    "lossWeights={'image_output_layer':0.1,\n",
    "          'text_output_layer':0.9  \n",
    "        }\n",
    "\n",
    "\n",
    "accuracy={\n",
    "    'image_output_layer':loss_func_image,\n",
    "    'text_output_layer': check_nonzero\n",
    "}\n",
    "\n",
    "ae_sep_output.compile(\n",
    "    optimizer=\"adam\", loss=losses, loss_weights=lossWeights, metrics=accuracy\n",
    ")\n",
    "\n",
    "es = EarlyStopping(\n",
    "    monitor=\"val_text_output_layer_loss\", mode=\"min\", verbose=1, patience=2\n",
    ")\n",
    "\n",
    "ae_history = ae_sep_output.fit(\n",
    "    [train_image_batch, train_text_batch],\n",
    "    [output_image_batch, output_text_batch],\n",
    "    epochs=10,\n",
    "    batch_size=25,\n",
    "    shuffle=True,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[es],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a716e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "592/592 [==============================] - 11s 18ms/step - loss: 51.9090 - image_output_layer_loss: 465.7723 - text_output_layer_loss: 5.9241 - image_output_layer_loss_func_image: 465.7723 - text_output_layer_check_nonzero: 46.6723 - val_loss: 49.7016 - val_image_output_layer_loss: 436.4905 - val_text_output_layer_loss: 6.7251 - val_image_output_layer_loss_func_image: 436.4905 - val_text_output_layer_check_nonzero: 46.5473\n",
      "Epoch 2/5\n",
      "592/592 [==============================] - 11s 18ms/step - loss: 47.1144 - image_output_layer_loss: 423.9874 - text_output_layer_loss: 5.2397 - image_output_layer_loss_func_image: 423.9874 - text_output_layer_check_nonzero: 46.6723 - val_loss: 55.2789 - val_image_output_layer_loss: 441.1127 - val_text_output_layer_loss: 12.4084 - val_image_output_layer_loss_func_image: 441.1127 - val_text_output_layer_check_nonzero: 46.4392\n",
      "Epoch 3/5\n",
      "592/592 [==============================] - 11s 18ms/step - loss: 55.8166 - image_output_layer_loss: 480.3064 - text_output_layer_loss: 8.6511 - image_output_layer_loss_func_image: 480.3064 - text_output_layer_check_nonzero: 46.6334 - val_loss: 41.5654 - val_image_output_layer_loss: 359.3868 - val_text_output_layer_loss: 6.2519 - val_image_output_layer_loss_func_image: 359.3868 - val_text_output_layer_check_nonzero: 46.5405\n",
      "Epoch 4/5\n",
      "592/592 [==============================] - 11s 18ms/step - loss: 41.7013 - image_output_layer_loss: 370.4573 - text_output_layer_loss: 5.1729 - image_output_layer_loss_func_image: 370.4573 - text_output_layer_check_nonzero: 46.6115 - val_loss: 60.9125 - val_image_output_layer_loss: 556.4233 - val_text_output_layer_loss: 5.8558 - val_image_output_layer_loss_func_image: 556.4234 - val_text_output_layer_check_nonzero: 46.4662\n",
      "Epoch 5/5\n",
      "592/592 [==============================] - 11s 18ms/step - loss: 36.9564 - image_output_layer_loss: 340.3987 - text_output_layer_loss: 3.2406 - image_output_layer_loss_func_image: 340.3987 - text_output_layer_check_nonzero: 46.5997 - val_loss: 35.6880 - val_image_output_layer_loss: 327.4654 - val_text_output_layer_loss: 3.2683 - val_image_output_layer_loss_func_image: 327.4654 - val_text_output_layer_check_nonzero: 46.4189\n"
     ]
    }
   ],
   "source": [
    "ae_history = ae_sep_output.fit(\n",
    "    [train_image_batch, train_text_batch],\n",
    "    [output_image_batch, output_text_batch],\n",
    "    epochs=5,\n",
    "    batch_size=25,\n",
    "    shuffle=True,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[es],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4dc34a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the inference model\n",
    "\n",
    "decoder_input = Input(shape=(256,))\n",
    "\n",
    "d_dense = ae_sep_output.get_layer(\"image_dense\")(decoder_input)\n",
    "d_reshape = ae_sep_output.get_layer(\"image_reshape\")(d_dense)\n",
    "d_conv1 = ae_sep_output.get_layer(\"idecode_conv1\")(d_reshape)\n",
    "d_norm1 = ae_sep_output.get_layer(\"idecode_norm1\")(d_conv1)\n",
    "d_relu1 = ae_sep_output.get_layer(\"idecode_relu1\")(d_norm1)\n",
    "d_conv2 = ae_sep_output.get_layer(\"idecode_conv2\")(d_relu1)\n",
    "d_norm2 = ae_sep_output.get_layer(\"idecode_norm2\")(d_conv2)\n",
    "d_relu2 = ae_sep_output.get_layer(\"idecode_relu2\")(d_norm2)\n",
    "d_image_output = ae_sep_output.get_layer(\"image_output_layer\")(d_relu2)\n",
    "\n",
    "t_dense = ae_sep_output.get_layer(\"tdecode_dense1\")(decoder_input)\n",
    "t_reshape = ae_sep_output.get_layer(\"text_reshape\")(t_dense)\n",
    "t_dense2 = ae_sep_output.get_layer(\"tdecode_dense2\")(t_reshape)\n",
    "d_text_output = ae_sep_output.get_layer(\"text_output_layer\")(t_dense2)\n",
    "\n",
    "decoder_model = Model(inputs=[decoder_input], outputs=[d_image_output, d_text_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bca7d108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Entire Model to disk\n",
      "Saved Encoder Model to disk\n",
      "Saved Decoder Model to disk\n"
     ]
    }
   ],
   "source": [
    "# saving the entire architecture model\n",
    "model_json = ae_sep_output.to_json()\n",
    "with open(\"../model/autoencoder_model_test.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "ae_sep_output.save_weights(\"../model/autoencoder_model_test.h5\")\n",
    "print(\"Saved Entire Model to disk\")\n",
    "\n",
    "# saving the encoder part\n",
    "model_json = encoding_model.to_json()\n",
    "with open(\"../model/encoder_model_test.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "encoding_model.save_weights(\"../model/encoder_model_test.h5\")\n",
    "print(\"Saved Encoder Model to disk\")\n",
    "# saving the encoder part\n",
    "\n",
    "model_json = decoder_model.to_json()\n",
    "with open(\"../model/decoder_model_test.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "decoder_model.save_weights(\"../model/decoder_model_test.h5\")\n",
    "print(\"Saved Decoder Model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fd8d23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a55e09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
